---
title: "template-quality-measure"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{template-quality-measure}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(handwriter)
library(ggplot2)
library(dendextend)
```

## Template data
```{r}
# load template
all_templates <- readRDS("~/Documents/CSAFE_template_quality_measure/template_seed100/data/all_templates.rds")
template <- all_templates[[1]]
rm(all_templates)
```

Plot cluster fill counts for template writers.
```{r}
template_data <- format_template_data(template)
plot_cluster_fill_counts(template_data, facet=TRUE)
```

## Evalute hierarchical model on template
```{r}
main_dir <- "/users/stephanie/Documents/CSAFE_template_quality_measure"

# fit model
model_proc_list <- process_batch_dir(input_dir = file.path(main_dir, "data", "model_images"),
                                     output_dir = file.path(main_dir, "data", "model_graphs"))

model_clusters <- get_clusterassignment(clustertemplate = template,
                                        input_dir = file.path(main_dir, "data", "model_graphs"))

model_data <- format_model_data(model_proc_list = model_clusters,
                                writer_indices = c(2,5),
                                doc_indices = c(7,18))

model <- fit_model(model_data = model_data,
                   num_iters = 100,
                   num_chains = 1)

# analyze questioned documents
questioned_proc_list <- process_batch_dir(input_dir = file.path(main_dir, "data", "questioned_images"),
                                          output_dir = file.path(main_dir, "data", "questioned_graphs"))

questioned_clusters <- get_clusterassignment(clustertemplate = template,
                                             input_dir = file.path(main_dir, "data", "questioned_graphs"))

questioned_data <- format_questioned_data(formatted_model_data = model_data,
                                          questioned_proc_list = questioned_clusters,
                                          writer_indices = c(2,5),
                                          doc_indices = c(7,18))
analysis <- analyze_questioned_documents(model_data = model_data,
                                         model = model,
                                         questioned_data = questioned_data,
                                         num_cores = 5)
analysis$posterior_probabilities
plot_posterior_probabilities(analysis)
```


## Hierarchical clustering
### Wrangle data
```{r}
df <- data.frame(writer = template$writers, doc = template$docnames, cluster = template$cluster, stringsAsFactors = FALSE)

# count graphs per cluster per document
df <- df %>% dplyr::group_by(writer, doc, cluster) %>% dplyr::summarize(count = dplyr::n())

# make a column for each cluster
df <- df %>% tidyr::pivot_wider(names_from = cluster, values_from = count, values_fill = 0)

# convert to matrix and use docnames as rownames
docnames <- df$doc
M <- df %>% dplyr::ungroup() %>% dplyr::select(-writer, -doc)
M <- as.matrix(M)
rownames(M) <- docnames
```

## Apply hierarchical clustering
```{r}
dist_methods <- c("euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski")
hclust_methods <- c("ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid")

method <- c()
results <- list()
for (i in 1:length(dist_methods)){
  
  # calculate distances between all pairs of documents
  D <- dist(df[,3:ncol(df)], method = dist_methods[i])
  
  for (j in 1:length(hclust_methods)){
    # store methods 
    method <- append(method, paste0("dist-", dist_methods[i], "_hclust-", hclust_methods[j]))
    
    # cluster
    hc <- hclust(D, method = hclust_methods[j])
    
    # evaluate
    cluster_cut <- cutree(hc, k=10)
    t <- table(cluster_cut, df$writer)
    
    # store table
    results <- append(results, list(t))
  }
}

# name tables
names(results) <- method

lapply(results, function(x) rowSums(x)) 
```




### Try normalizing the data
```{r}
# normalize
standardize <- function(x){
  x <- (x - min(x))/(max(x) - min(x))
  return(x)
}

X <- apply(df[,3:ncol(df)], 2, function(x) standardize(x))

method2 <- c()
results2 <- list()
for (i in 1:length(dist_methods)){
  
  # calculate distances between all pairs of documents
  D <- dist(df[,3:ncol(df)], method = dist_methods[i])
  
  for (j in 1:length(hclust_methods)){
    # store methods 
    method2 <- append(method2, paste0("dist-", dist_methods[i], "_hclust-", hclust_methods[j]))
    
    # cluster
    hc <- hclust(D, method = hclust_methods[j])
    
    # evaluate
    cluster_cut <- cutree(hc, k=10)
    t <- table(cluster_cut, df$writer)
    
    # store table
    results2 <- append(results2, list(t))
  }
}

# name tables
names(results2) <- method2

lapply(results2, function(x) rowSums(x)) 
```

```{r}
df <- as.data.frame(t)
colnames(df) <- c("cluster", "writer", "count") 
df <- df[,c(2,1,3)]  # rearrange columns
```


## Multi-class LDA
```{r}
set.seed(123)

# drop doc column, outlier cluster (constant within-in group), first cluster (constant within-in group)
lda_template <- template_data$cluster_fill_counts[,-c(2, 3, 4)]

# add index row
lda_template <- cbind(index = 1:nrow(lda_template), lda_template)

# change writer to factor (otherwise caret tries to perform regression)
lda_template <- lda_template %>% dplyr::mutate(writer = factor(writer))

# split train and test sets
train_indx <- lda_template %>% 
  dplyr::group_by(writer) %>% 
  dplyr::slice_sample(n=2)  %>% 
  dplyr::pull(index)
train <- lda_template[train_indx, ]
test <- lda_template[-train_indx, ]

# drop index column
train <- train[,-1]
test <- test[,-1]

fit_car <- caret::train(writer~., data=train, method="lda")

pred_car <- predict(fit_car, test[,-1])
data.frame(original = test$writer, pred = pred_car)
```

